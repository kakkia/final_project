# -*- coding: utf-8 -*-
"""streamlit_app.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fdw1LHuhS0L7erMhq6DqBSeUFjl5gQBi
"""

pip install streamlit

import tensorflow as tf
import numpy as np
import streamlit as st
from PIL import Image, ImageOps
import requests
from io import BytesIO
import matplotlib.pyplot as plt
from tensorflow.keras import preprocessing
from tensorflow.keras import models
import tensorflow_hub as hub

#import streamlit as st

#st.title("Image Classification")

#st.header("Italian Gesture Classification Example")

#st.text("Upload a picture of your gesture for image classification")

# ----Global settings ------

gesture = Image.open('img_streamlit/gestures.png')

st.set_page_config(page_title="Italian Gestures Classification", page_icon=gesture, layout="centered", initial_sidebar_state="auto", menu_items=None)

st.set_option('deprecation.showfileUploaderEncoding', False)

s = f"""<style> div.stButton > button:first-child {{background-color: #D9D206;}} .sidebar .sidebar-content {{background-color: "#0AA2B8";}} <style>"""

st.markdown(s, unsafe_allow_html=True)

# ----- SIDEBAR ------

st.sidebar.header('Upload your picture')
file_uploaded = st.sidebar.file_uploader('',type = ['jpg','jpeg','png'])

st.sidebar.header('Predict Gestures')
pred_asana = st.sidebar.button('Predict Gesture')

#st.sidebar.header('Plot Confusion Matrix')
#cm_plot = st.sidebar.button('Plot Matrix') 

# ---- Main Output -----

col1, mid, col2 = st.columns([2,1,20])
with col1:
    st.image(yoga_img, width=80)
with col2:
    st.title('Italian Gestures Classification')
 
st.write("""
When speaking Italian it is mandatory that you express your emotions with your hands.\n
My app helps you to use the appropriate gesture when you just can't find the words.\n
So far I have trained a model to predict three key expressions: "WHAT?!", "RUN BEFORE I CATCH YOU" and "EXCELLENT". How about giving us a hand to continue?\n
""")
    
classes = ['what', 'shoo', 'perfect']
all_preds = ['0.2','0.2','0.2']

def predict_class(image):
    classify_model = tf.keras.models.load_model('models/data_aug_model')
    test_image = image.resize((150,150))
    test_image = tf.keras.preprocessing.image.img_to_array(test_image)
    test_image = np.expand_dims(test_image, axis=0)
    test_image /= 255.
    prediction = classify_model.predict(test_image)
    winner = np.argmax(prediction)
    all_preds = prediction[0]
    predicted_class = f"This gesture is predicted as {classes[winner]}"
    return predicted_class, all_preds, winner

def gesture_description(winner):
    
    if winner == 0:
        text = st.markdown("""<h4>What?</h4>
        <ul>
        <li>The tips of the fingers of one hand are brought sharply together to form an upward-pointing cone.</li>
        <li>The hand can either be held motionless or be shaken more or less violently up and down.</li>
        <li>How fast you move it, depends on the degree of impatience expressed.</li>
        <li>Don't be afraid of using it when someone tells you something unexpectedly upsetting.</li>
        </ul>""", True)
    elif winner == 1:
        text = st.markdown("""<h4>Shoo</h4>
        <ul>
        <li>The flat hand slowly moves as to follow the people you are addressing.</li>
        <li></li>
        <li>In parenting, it can be moved up and down to suggest you will be punished for what you did.</li>
        <li></li>
        </ul>""",True)
    else:
        text = st.markdown("""<h4>Excellent</h4>
        <ul>
        <li>This gesture express both approval and hearty satifaction.</li>
        <li>It is typical of the good-natured and contented gourmet.</li>
        <li>Use it anythime you find something delicious, or you completely agree with someone.</li>
        <li>And it's not just for food.</li>
        </ul>""",True)
    return text

#def confusion_ma(y_true, y_pred, class_names):
#    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
#    cm = confusion_matrix(y_true, y_pred, normalize=None)
#    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
#    disp.plot(cmap=plt.cm.Blues)
#    return plt.show()

if file_uploaded is not None:
    image = Image.open(file_uploaded)
else:
    st.warning('Please upload an Image to predict your gesture')
    
if pred_gesture:   
    figure = plt.figure()
    plt.imshow(image)
    plt.axis('off')
    result, all_preds, winner = predict_class(image)
    st.subheader(result)
    
    col_img, col_text = st.columns(2)
    
    with col_img:
        st.pyplot(figure) 
    
    with col_text:
      gesture_description(winner)
        
    st.subheader('Predicted values for each of the three Italian gestures')
    col1, col2, col3 = st.columns(3)
    
    image1 = Image.open('img_streamlit/what.jpg')
    image2 = Image.open('img_streamlit/shoo.jpg')
    image3 = Image.open('img_streamlit/perfect.jpg')

    with col1:
        st.subheader(round(all_preds[0] * 100,2))
        st.image(image1, caption='What')

    with col2:
        st.subheader(round(all_preds[1] * 100,2))
        st.image(image2, caption='Shoo')

    with col3:
        st.subheader(round(all_preds[2] * 100,2))
        st.image(image3, caption='Excellent')    
    
else:
    st.write('')
    
# cm_image = Image.open('confusion_matrix.jpg')

#if cm_plot:
#    st.subheader('Confusion Matrix')
#    st.write('The Confusion Matrix shows the overall performance of the Convolutional Neural Network in predicting the correct Asana from a Test Image. The model has been trained with 1147 images of 5 different Yoga Asanas. The Model has been evaluated with 156 Test images. The correctly predicted Asanas can be seen in the diagonal of the confusion matrix, where the predicted and the true labels are the same. Also the wrongly predicted asanas can be seen where the predicted label is not equal to the true label. The trained model predicts the yoga asanas with an accuracy of 91%.')
#    st.image(cm_image, width = 600)
#else:
#    st.write()